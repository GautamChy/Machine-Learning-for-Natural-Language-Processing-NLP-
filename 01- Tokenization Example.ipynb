{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55665d92-fb6a-447b-b0d9-19b4f34fe55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7cad3ff5-414c-4e2e-a5e2-6318b19a12ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/maclap.in/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization  ( Search on google i.e,  www.nltk.tokenize package.com)\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1926fcf9-ff9f-446c-837b-63168d7cf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= \"\"\"Hellow I am Gautam Chaudhary and currenctly I am learning Generative full course.\n",
    "    To become expert in NLP! then I h've to learn from Krish Naik.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "386e2c34-8788-49a9-aca3-347df013b89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellow I am Gautam Chaudhary and currenctly I am learning Generative full course.\n",
      "    To become expert in NLP! then I h've to learn from Krish Naik.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4d4c0bf-b087-413a-81ea-4c7b96cd45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "## Paragraph into sentence  ( Search on google i.e,  www.nltk.tokenize package.com)\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d27529f6-a1bc-471b-abf2-9d32a7d14454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellow I am Gautam Chaudhary and currenctly I am learning Generative full course.',\n",
       " 'To become expert in NLP!',\n",
       " \"then I h've to learn from Krish Naik.\"]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f6bb9-b9b7-4dd3-8ba9-c8d9f99731e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "504102bc-a3a0-4ed1-be07-019c23ed9cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=sent_tokenize(corpus)\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5578095-972e-4135-a5d9-eed435410077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellow I am Gautam Chaudhary and currenctly I am learning Generative full course.\n",
      "To become expert in NLP!\n",
      "then I h've to learn from Krish Naik.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae5a21-6ae1-4f7e-8040-39e6ae51b50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007cd198-e5d5-4010-b8bd-475474e2ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "..........................................................................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b403a3cc-cbbd-440b-9efe-404fcaa5a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Pragraph into word\n",
    "## sentence into word\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "806495e7-a222-4a57-9854-084e961c8cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellow',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Gautam',\n",
       " 'Chaudhary',\n",
       " 'and',\n",
       " 'currenctly',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Generative',\n",
       " 'full',\n",
       " 'course',\n",
       " '.',\n",
       " 'To',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'then',\n",
       " 'I',\n",
       " 'h',\n",
       " \"'ve\",\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " '.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96fbd1-d549-4272-b64a-80f78aa7ccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05d95445-457a-47ea-bdd3-411440c9d1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=word_tokenize(corpus)\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67001aed-d735-4674-aeb9-a8d36655f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hellow']\n",
      "['I']\n",
      "['am']\n",
      "['Gautam']\n",
      "['Chaudhary']\n",
      "['and']\n",
      "['currenctly']\n",
      "['I']\n",
      "['am']\n",
      "['learning']\n",
      "['Generative']\n",
      "['full']\n",
      "['course']\n",
      "['.']\n",
      "['To']\n",
      "['become']\n",
      "['expert']\n",
      "['in']\n",
      "['NLP']\n",
      "['!']\n",
      "['then']\n",
      "['I']\n",
      "['h']\n",
      "[\"'ve\"]\n",
      "['to']\n",
      "['learn']\n",
      "['from']\n",
      "['Krish']\n",
      "['Naik']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbae0f-840c-4253-91ca-2d4831d8d3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14d8bf-094c-4bdb-9cfe-2dd5fced4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "...................................................................................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5a02d42-9883-4ea2-b24e-4211664e7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Pragraph into wordpunct\n",
    "## sentence into wordpunct\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "829c9c1f-ce61-4a5d-99d2-fed0cde91d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellow',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Gautam',\n",
       " 'Chaudhary',\n",
       " 'and',\n",
       " 'currenctly',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Generative',\n",
       " 'full',\n",
       " 'course',\n",
       " '.',\n",
       " 'To',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'then',\n",
       " 'I',\n",
       " 'h',\n",
       " \"'\",\n",
       " 've',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " '.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fda9f-30ee-4502-a534-2723929ab77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f1744-5e9c-45a6-b6c3-56035ff7693a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7f18b9d-a448-4486-b6c3-19afdb5ea2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "52d67f64-2a98-4369-8f0f-927c32a3f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "40b560c2-8f7b-461d-8e22-d4534f495bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellow',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Gautam',\n",
       " 'Chaudhary',\n",
       " 'and',\n",
       " 'currenctly',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Generative',\n",
       " 'full',\n",
       " 'course.',\n",
       " 'To',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'then',\n",
       " 'I',\n",
       " 'h',\n",
       " \"'ve\",\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " '.']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab25e2-b372-4946-9a6e-85616db3d960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
